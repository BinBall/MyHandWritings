# 1.	ç›‘ç£å­¦ä¹ Supervised Learning

![img](Image/006tNc79gy1fzkcb2xr3zj30jp0baabo.jpg)

ç›‘ç£å­¦ä¹ çš„å…³é”®ç‰¹å¾åœ¨äº**ç»™äºˆå­¦ä¹ ç®—æ³•ç¤ºä¾‹**ï¼Œå³**Learns from being given "right answers**"

> ç›‘ç£å­¦ä¹ æ˜¯æŒ‡ï¼šåˆ©ç”¨ä¸€ç»„å·²çŸ¥ç±»åˆ«çš„æ ·æœ¬è°ƒæ•´[åˆ†ç±»å™¨](https://baike.baidu.com/item/åˆ†ç±»å™¨/3317404?fromModule=lemma_inlink)çš„[å‚æ•°](https://baike.baidu.com/item/å‚æ•°/5934974?fromModule=lemma_inlink)ï¼Œä½¿å…¶è¾¾åˆ°æ‰€è¦æ±‚æ€§èƒ½çš„è¿‡ç¨‹
>
> ç›‘ç£å­¦ä¹ æ˜¯ä»æ ‡è®°çš„è®­ç»ƒæ•°æ®æ¥æ¨æ–­ä¸€ä¸ªåŠŸèƒ½çš„æœºå™¨å­¦ä¹ ä»»åŠ¡
>
> ç›‘ç£å­¦ä¹ æ˜¯å­¦ä¹ å‡½æ•°çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œè¯¥å‡½æ•°åŸºäºç¤ºä¾‹è¾“å…¥ â€“ è¾“å‡ºå¯¹å°†è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºã€‚å®ƒæ¨æ–­å‡ºä¸€ä¸ªå‡½æ•°æ ‡è®°çš„è®­ç»ƒæ•°æ®ç”±ä¸€ç»„è®­ç»ƒæ ·ä¾‹ç»„æˆã€‚æœ€ä½³æ–¹æ¡ˆå°†å…è®¸ç®—æ³•æ­£ç¡®åœ°ç¡®å®šçœ‹ä¸è§çš„å®ä¾‹çš„ç±»æ ‡ç­¾ã€‚è¿™è¦æ±‚å­¦ä¹ ç®—æ³•ä»¥â€œåˆç†â€çš„æ–¹å¼**ä»è®­ç»ƒæ•°æ®æ¨å¹¿åˆ°çœ‹ä¸è§çš„æƒ…å†µ**ã€‚

ç›‘ç£å­¦ä¹ æœ‰ä¸¤å¤§ä¸»è¦ä»»åŠ¡ï¼š

1. **å›å½’Regression**ï¼š**é¢„æµ‹è¿ç»­çš„ã€å…·ä½“çš„æ•°å€¼ï¼Œå³predict number**
2. **åˆ†ç±»Classification**ï¼š**å¯¹å„ç§äº‹ç‰©åˆ†é—¨åˆ«ç±»ï¼Œç”¨äºç¦»æ•£å‹é¢„æµ‹ï¼Œå³predict class/category**ï¼Œåˆ†ç±»äº§ç”Ÿä¸åŒçš„**ç±»Class**(æˆ–ç§°**ç±»åˆ«Category**)



## 1.1	å›å½’Regression

### 1.1.1	çº¿æ€§å›å½’Linear Regression

çº¿æ€§å›å½’ä½¿ç”¨ä¸€æ¡ç›´çº¿æ‹Ÿåˆæ•°æ®

<img src="Image/image-20220912135856076.png" alt="image-20220912135856076" style="zoom:80%;" />

|               æè¿° |                             æè¿°                             | Pythonå¯¹åº”å˜é‡ |
| -----------------: | :----------------------------------------------------------: | -------------: |
|                  ğ‘ |                         æ ‡é‡ï¼Œä¸åŠ ç²—                         |                |
|                  ğš |                          å‘é‡ï¼ŒåŠ ç²—                          |                |
|       **å›å½’å‡½æ•°** |                                                              |                |
|                  ğ± |          è®­ç»ƒæ ·ä¾‹ç‰¹å¾å€¼(æœ¬å®éªŒä¸­ï¼Œå•ä½ä¸ºåƒå¹³æ–¹è‹±å°º)          |      `x_train` |
|                  ğ² |            è®­ç»ƒæ ·ä¾‹ç›®æ ‡å€¼(æœ¬å®éªŒä¸­ï¼Œå•ä½ä¸ºåƒç¾å…ƒ)            |      `y_train` |
| $ğ‘¥^{(ğ‘–)}, ğ‘¦^{(ğ‘–)}$ |                       ç¬¬$i$ä¸ªè®­ç»ƒæ ·ä¾‹                        |   `x_i`, `y_i` |
|                $m$ |                          è®­ç»ƒæ ·ä¾‹æ•°                          |            `m` |
|                $w$ |                      å˜é‡ï¼š$weight$æƒé‡                      |            `w` |
|                  ğ‘ |                   å˜é‡ï¼š$bias$åç½®é¡¹/åå·®                    |            `b` |
| $ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})$ | æ¨¡å‹é€šè¿‡$w,b$å‚æ•°åŒ–ï¼Œå¯¹$x^{(i)}$çš„æ¨æ¼”ç»“æœ$ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(ğ‘–)})=ğ‘¤ğ‘¥^{(ğ‘–)}+ğ‘$ |         `f_wb` |

å¯¹å›å½’å‡½æ•°$ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})$ï¼Œå…¶ä¸­åªåŒ…å«ä¸€ä¸ªç‰¹å¾å€¼$x$ï¼Œå¯¹è¿™æ ·çš„å›å½’å‡½æ•°ï¼Œç§°ä¸º**å•ä¸€çº¿æ€§å›å½’Univariate Linear Regression**



### 1.1.2	æˆæœ¬å‡½æ•°Cost Function

ä¸ºäº†æ‰¾åˆ°æ›´åˆé€‚çš„$w,b$ï¼Œä½¿å¾—æ‹Ÿåˆçš„å›å½’å‡½æ•°é¢„æµ‹å€¼$y^{(i)}$æ›´æ¥è¿‘çœŸå®ç›®æ ‡å€¼$y^{(i)}$ï¼Œé¦–å…ˆè¦è¡¡é‡å‡½æ•°ä¸è®­ç»ƒæ•°æ®çš„æ‹Ÿåˆç¨‹åº¦ï¼Œä¸ºæ­¤å¼•å‡ºæˆæœ¬å‡½æ•°Cost Function

åœ¨æˆæœ¬å‡½æ•°ä¸­ï¼Œé¢„æµ‹å€¼ä¸ç›®æ ‡å€¼ä¹‹å·®ç§°ä¸º**è¯¯å·®*Error***ï¼Œå³***Error*** $=\ \hat{y} - y$ï¼Œæˆæœ¬å‡½æ•°æ•°å­¦è¡¨è¾¾å¦‚ä¸‹ï¼š
$$
J(w,b) = \frac{1}{2m}\sum_{i=1}^m(\hat{y}^{(i)}-y^{(i)})^2
$$
å…¶ä¸­$m$æ˜¯è®­ç»ƒæ ·ä¾‹æ•°ï¼Œä¸Šå¼ä¹Ÿç§°ä¸º**å¹³æ–¹è¯¯å·®æˆæœ¬å‡½æ•°*Squared Error Cost Function***ï¼Œå®ƒæœ€å¸¸ç”¨äºçº¿æ€§å›å½’

å¦å¤–ï¼Œç”±äº$\hat{y} = ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})$ï¼Œå› æ­¤ä¸Šå¼ä¹Ÿå¯å†™ä½œï¼š
$$
J(w,b) = \frac{1}{2m}\sum_{i=1}^m(ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})-y^{(i)})^2
$$
ç”±äºæ„å»ºæ¨¡å‹çš„ç›®çš„æ˜¯è·å¾—æ‹Ÿåˆç¨‹åº¦æ›´é«˜çš„æ¨¡å‹ï¼Œå› æ­¤å…¶æˆæœ¬å‡½æ•°å€¼åº”å½“å°½å¯èƒ½å°ï¼Œå³ç›®æ ‡ä¸ºæœ€å°ä»£ä»·å‡½æ•°ï¼š
$$
\mathop{minimize}\limits_{w,b}J(w,b)
$$
<img src="Image/image-20220912235419361.png" alt="image-20220912235419361" style="zoom: 50%;" />

å…¶ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
import numpy as np

#Function to calculate the cost
def compute_cost(x, y, w, b):
   
    m = x.shape[0] 
    cost = 0
    
    for i in range(m):
        f_wb = w * x[i] + b
        cost = cost + (f_wb - y[i])**2
    total_cost = 1 / (2 * m) * cost

    return total_cost
```



### 1.1.3	æ¢¯åº¦ä¸‹é™Gradient Descent

ä¸ºäº†é«˜æ•ˆæ‰¾åˆ°æ›´å°çš„ä»£ä»·å‡½æ•°Cost Functionï¼Œæˆ‘ä»¬éœ€è¦å¼•å…¥æ–°çš„ç®—æ³•ï¼š**æ¢¯åº¦ä¸‹é™Gradient Descent**

**æ¢¯åº¦ä¸‹é™ç®—æ³•å¯ç”¨äºæœ€å°åŒ–ä»»ä½•å‡½æ•°**

#### 1.1.3.1	æ€è·¯

ä»¥æˆæœ¬å‡½æ•°$J(w,b)$ä¸ºä¾‹ï¼Œæˆ‘ä»¬æœŸæœ›æ‰¾åˆ°æœ€å°æˆæœ¬å‡½æ•°$\mathop{min}\limits_{w,b}J(w,b)$

- èµ·å§‹ï¼Œæˆ‘ä»¬åº”æœ‰$w,b$åˆå§‹å€¼ï¼Œæ­¤å¤„å‡å®š$w=0,b=0$
- æ¥ä¸‹æ¥ï¼Œä¸æ–­æ”¹å˜$w,b$çš„å€¼ä»¥å‡å°æˆæœ¬å‡½æ•°$J(w,b)$
- ç›´åˆ°æˆ‘ä»¬æ‰¾åˆ°æˆ–æ¥è¿‘æœ€å°å€¼

å‡å°æˆæœ¬å‡½æ•°$J(w,b)$çš„æ€è·¯æ˜¯ï¼šåœ¨æ¯ä¸ªé˜¶æ®µï¼Œå¯»æ‰¾å‘¨å›´ç‚¹ä¸­æˆæœ¬å‡½æ•°å€¼æœ€å°çš„ç‚¹ï¼Œåˆ°å¾ªç¯ç»“æŸï¼Œå°†æ‰¾åˆ°ä¸€ä¸ªå±€éƒ¨æœ€ä¼˜è§£

å¤šæ¬¡å°è¯•å¹¶æ¯”è¾ƒå±€éƒ¨æœ€ä¼˜è§£ï¼Œå¯èƒ½æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£

<img src="Image/image-20220913144934515.png" alt="image-20220913144934515" style="zoom:50%;" />

æ¢¯åº¦ä¸‹é™æ•°å­¦è¡¨è¾¾å¦‚ä¸‹ï¼š
$$
w\ =\ w-\alpha \frac{\partial{}}{\partial{w}}J(w,b),\\ 
b\ = \ b-\alpha \frac{\partial{}}{\partial{b}}J(w,b)
$$
å…¶ä¸­$\alpha$æ˜¯**å­¦ä¹ ç‡Learning rate**ï¼Œå®ƒæ˜¯ä¸€ä¸ªä»‹äº0-1é—´çš„æ­£æ•°ï¼Œæ§åˆ¶æ›´æ–°æ¨¡å‹å‚æ•°(å¦‚$w,b$)æ—¶çš„æ­¥é•¿ï¼Œå³è¡¨ç¤ºæ¢¯åº¦ä¸‹é™çš„ç¨‹åº¦å¤§å°

åœ¨æ¯ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬éœ€è¦åŒæ­¥æ›´æ–°$w,b$(Simultaneously update $w,b$)ï¼Œè¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¯¹åº”çš„ä»£ç å®ç°ä¸­åº”å†™ä½œï¼š
$$
tmp\_w\ =\ w-\alpha \frac{\partial{}}{\partial{w}}J(w,b)\\ 
tmp\_b\ = \ b-\alpha \frac{\partial{}}{\partial{b}}J(w,b)\\
w\ =\ tmp\_w\\
b\ =\ tmp\_b
%è¿™æ ·æ‰èƒ½ä¿è¯è®¡ç®—bæ—¶ï¼ŒJ(w,b)ä¸å˜
$$
æ¢¯åº¦ä¸‹é™ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
import numpy as np

def compute_gradient(x, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      x (ndarray (m,)): Data, m examples 
      y (ndarray (m,)): target values
      w,b (scalar)    : model parameters  
    Returns
      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w
      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     
     """
    
    # Number of training examples
    m = x.shape[0]    
    dj_dw = 0
    dj_db = 0
    
    for i in range(m):  
        f_wb = w * x[i] + b 
        dj_dw_i = (f_wb - y[i]) * x[i] 
        dj_db_i = f_wb - y[i] 
        dj_db += dj_db_i
        dj_dw += dj_dw_i 
    dj_dw = dj_dw / m 
    dj_db = dj_db / m 
        
    return dj_dw, dj_db


def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): 
    """
    Performs gradient descent to fit w,b. Updates w,b by taking 
    num_iters gradient steps with learning rate alpha
    
    Args:
      x (ndarray (m,))  : Data, m examples 
      y (ndarray (m,))  : target values
      w_in,b_in (scalar): initial values of model parameters  
      alpha (float):     Learning rate
      num_iters (int):   number of iterations to run gradient descent
      cost_function:     function to call to produce cost
      gradient_function: function to call to produce gradient
      
    Returns:
      w (scalar): Updated value of parameter after running gradient descent
      b (scalar): Updated value of parameter after running gradient descent
      J_history (List): History of cost values
      p_history (list): History of parameters [w,b] 
      """
    
    w = copy.deepcopy(w_in) # avoid modifying global w_in
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    p_history = []
    b = b_in
    w = w_in
    
    for i in range(num_iters):
        # Calculate the gradient and update the parameters using gradient_function
        dj_dw, dj_db = gradient_function(x, y, w, b)     

        # Update Parameters using equation (3) above
        b = b - alpha * dj_db                            
        w = w - alpha * dj_dw                            

        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            J_history.append( cost_function(x, y, w, b))
            p_history.append([w,b])
        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters/10) == 0:
            print(f"Iteration {i:4}: Cost {J_history[-1]:0.2e} ",
                  f"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  ",
                  f"w: {w: 0.3e}, b:{b: 0.5e}")
 
    return w, b, J_history, p_history #return w and J,w history for graphing
```



#### 1.1.3.2	å­¦ä¹ ç‡Learning rate

å¯¹æˆæœ¬å‡½æ•°$J(w)$è¿›è¡Œæ¢¯åº¦ä¸‹é™ï¼Œå…¶æ•°å­¦è¡¨è¾¾ä¸º$w\ = \ w-\alpha \frac{d}{dw}J(w)$ï¼Œ$\alpha$ä¸ºå…¶**å­¦ä¹ ç‡*Learning Rate***

è‹¥å­¦ä¹ ç‡$\alpha$å–å€¼è¿‡å°ï¼Œåˆ™æ¢¯åº¦ä¸‹é™é€Ÿåº¦è¿‡æ…¢ï¼Œéœ€è¦è¿‡å¤šçš„è®¡ç®—é˜¶æ®µ

è‹¥å­¦ä¹ ç‡$\alpha$å–å€¼è¿‡å¤§ï¼Œåˆ™æ¢¯åº¦ä¸‹é™é€Ÿåº¦è¿‡å¿«ï¼Œå¯èƒ½ä¼šè·³è¿‡æœ€å°å€¼(**è¿‡å†²*Overshoot***)ï¼Œç”šè‡³æ¢¯åº¦ä¸‹é™æ— æ³•**æ”¶æ•›*Converge***ï¼Œè¿˜å¯ä»¥ä¼šå¯¼è‡´å‡½æ•°**å‘æ•£*Diverge***

<img src="Image/image-20220913151453252.png" alt="image-20220913151453252" style="zoom:50%;" />

å½“å­¦ä¹ ç‡$\alpha$å€¼å›ºå®šæ—¶ï¼Œç”±äºæˆ‘ä»¬æ¯ä¸ªé˜¶æ®µéƒ½åœ¨å‡å°æˆæœ¬å‡½æ•°ï¼Œå› æ­¤å…¶å¯¼æ•°ä¼šè¶Šæ¥è¶Šå°ï¼Œå³æ¯ä¸ªé˜¶æ®µçš„æ›´æ–°æ­¥é•¿è¶Šæ¥è¶Šå°ï¼Œæœ€ç»ˆæˆ‘ä»¬å°†æ‰¾åˆ°ä¸€ä¸ªå±€éƒ¨æœ€å°å€¼ï¼š

<img src="Image/image-20220913151908944.png" alt="image-20220913151908944" style="zoom:50%;" />

#### 1.1.3.3	çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•â€”â€”æ‰¹é‡æ¢¯åº¦ä¸‹é™

å¯¹çº¿æ€§å›å½’æ¨¡å‹Linear Regression Model$f_{w,b}(x)=wx+b$ï¼Œæœ‰æˆæœ¬å‡½æ•°Cost function $J(w,b) = \frac{1}{2m}\sum_{i=1}^m(ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})-y^{(i)})^2$,å…¶æ¢¯åº¦ä¸‹é™ç®—æ³•Gradient Descent Algorithmä¸ºï¼š
$$
w\ =\ w-\alpha \frac{\partial{}}{\partial{w}}J(w,b)\ =\ w-\alpha \frac{1}{m}\sum_{i=1}^m(ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})-y^{(i)})x^{(i)},\\ 
b\ = \ b-\alpha \frac{\partial{}}{\partial{b}}J(w,b)\ =\ b-\alpha\frac{1}{m}\sum_{i=1}^m(ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})-y^{(i)})
$$
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ¢¯åº¦ä¸‹é™ç®—æ³•è¢«ç§°ä¸º**æ‰¹é‡æ¢¯åº¦ä¸‹é™*Batch Gradient Descent***ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ¯ä¸ªæ¢¯åº¦ä¸‹é™é˜¶æ®µä¸­éƒ½ä½¿ç”¨äº†å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†



### 1.1.4	å¤šå…ƒçº¿æ€§å›å½’Multiple Linear Regression

ä¸ºäº†æ›´å‡†ç¡®åœ°é¢„æµ‹ç›®æ ‡å€¼$y$ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥å¤šä¸ªç‰¹å¾$x$ï¼Œè®°ä¸º$x_1,x_2,...,x_n$

å› æ­¤æˆ‘ä»¬ä½¿ç”¨$x_j$è¡¨ç¤ºè®­ç»ƒæ ·ä¾‹çš„ç¬¬$j$ä¸ªç‰¹å¾å€¼ï¼Œ$n$è¡¨ç¤ºç‰¹å¾æ•°é‡ï¼Œ$\vec{x}^{(i)}$è¡¨ç¤ºç¬¬$i$ä¸ªè®­ç»ƒæ ·ä¾‹çš„ç‰¹å¾å€¼ï¼Œ$x_j^{(i)}$è¡¨ç¤ºç¬¬$i$ä¸ªè®­ç»ƒæ ·ä¾‹çš„ç¬¬$j$ä¸ªç‰¹å¾å€¼ï¼š![image-20220913190014405](Image/image-20220913190014405.png)

å¯¹ä¹‹å‰çš„çº¿æ€§å›å½’æ–¹ç¨‹$f_{w,b}(x)=wx+b$ï¼Œå¼•å…¥å¤šå…ƒç‰¹å¾åæœ‰ï¼š
$$
f_{\vec{w},b}(\vec{x})=w_1x_1+w_2x_2+...+w_nx_n+b
$$
å…¶ä¸­$\vec{w}=\left[ w_1,w_2,...,w_n \right],\vec{x}=\left[ x_1,x_2,...,x_n \right]$ï¼Œå› æ­¤å¯å¾—ï¼š
$$
f_{\vec{w},b}(\vec{x})=\vec{w} \cdot \vec{x} + b=w_1x_1+w_2x_2+...+w_nx_n+b=(\sum_{j=1}^{n}{w_jx_j})+b
$$
å…¶ä¸­$\vec{w} \cdot \vec{x}$è¡¨ç¤ºå‘é‡$\vec{w}$ ä¸å‘é‡$\vec{x}$çš„**ç‚¹ç§¯*Dot Product***ï¼Œ**ç‚¹ç§¯è¦æ±‚ä¸¤å‘é‡å…ƒç´ ä¸ªæ•°ç›¸åŒ**



#### 1.1.4.1	å‘é‡åŒ–/çŸ¢é‡åŒ–Vectorization

çŸ¢é‡åŒ–èƒ½å¤Ÿ**ç®€åŒ–ä»£ç **ï¼Œæ›´å¥½è¡¨è¾¾æ€æƒ³ï¼Œå¹¶**åŠ å¿«è¿ç®—é€Ÿåº¦**ï¼Œå¯¹ä¸Šè¿°å¤šå…ƒçº¿æ€§å›å½’æ–¹ç¨‹ï¼Œå…¶ä»£ç è¡¨è¿°å¦‚ä¸‹ï¼š

```python
import numpy as np

w = np.array([1, 2, 3])
b = 1
x = np.array([2, 3, 4])
f_wb = lambda w,x,b: np.dot(w,x) + b
```

çŸ¢é‡åŒ–ä¹‹æ‰€ä»¥èƒ½åŠ å¿«è¿ç®—é€Ÿåº¦ï¼Œæ˜¯å› ä¸ºå®ƒèƒ½ä½¿ä¸¤å‘é‡ä¸­å„å…ƒç´ é—´**å¹¶è¡Œè¿ç®—*Parallel Computing***ï¼Œè€Œä¸æ˜¯ä¾æ¬¡è¿ç®—ï¼š

![image-20220913193734203](Image/image-20220913193734203.png)

#### 1.1.4.2	å¤šå…ƒçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•

åœ¨ä¸€å…ƒçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œæœ‰ï¼š
$$
w\ =\ w-\alpha \frac{\partial{}}{\partial{w}}J(w,b),\\ 
b\ = \ b-\alpha \frac{\partial{}}{\partial{w}}J(w,b),\\
J(w,b) = \frac{1}{2m}\sum_{i=1}^m(ğ‘“_{ğ‘¤,ğ‘}(ğ‘¥^{(i)})-y^{(i)})^2
$$
å¯¹åº”çš„ï¼Œåœ¨å¤šå…ƒçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸­ï¼Œæœ‰ï¼š
$$
w_j\ =\ w_j-\alpha \frac{\partial{}}{\partial{w_j}}J(w_1,w_2,...,w_n,b),\\ 
b\ = \ b-\alpha \frac{\partial{}}{\partial{b}}J((w_1,w_2,...,w_n,b),\\
J(\vec{w},b) = f_{\vec{w},b}(\vec{x}) - y = \frac{1}{2m}[(\sum_{i=1}^{n}(w_ix_i)+b) - y]^2
$$
å…¶ä¸­$m$ä¸ºè®­ç»ƒæ ·ä¾‹æ•°ï¼Œ$n$ä¸ºæ ·ä¾‹çš„ç‰¹å¾æ•°ï¼Œå°†ä¸Šå¼å±•å¼€ï¼Œå¾—ï¼š
$$
w_1 = w_1 - \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_1^{(i)}\\
\vdots\\
w_n = w_n - \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})x_n^{(i)}\\
b = b - \alpha \frac{1}{m} \sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})\\
$$
ç”±æ­¤å¯å¾—ï¼Œè®¡ç®—å¤šå…ƒçº¿æ€§å›å½’æ–¹ç¨‹çš„æˆæœ¬å‡½æ•°ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
import numpy as np


def compute_cost(X, y, w, b): 
    """
    compute cost
    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters  
      b (scalar)       : model parameter
      
    Returns:
      cost (scalar): cost
    """
    m = X.shape[0]
    cost = 0.0
    for i in range(m):                                
        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)
        cost = cost + (f_wb_i - y[i])**2       #scalar
    cost = cost / (2 * m)                      #scalar    
    return cost
```

è€Œè®¡ç®—æ¢¯åº¦çš„å‡½æ•°ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
import numpy as np


def compute_gradient(X, y, w, b): 
    """
    Computes the gradient for linear regression 
    Args:
      X (ndarray (m,n)): Data, m examples with n features
      y (ndarray (m,)) : target values
      w (ndarray (n,)) : model parameters  
      b (scalar)       : model parameter
      
    Returns:
      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w. 
      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b. 
    """
    m,n = X.shape           #(number of examples, number of features)
    dj_dw = np.zeros((n,))
    dj_db = 0.

    for i in range(m):                             
        err = (np.dot(X[i], w) + b) - y[i]   
        for j in range(n):                         
            dj_dw[j] = dj_dw[j] + err * X[i, j]    
        dj_db = dj_db + err                        
    dj_dw = dj_dw / m                                
    dj_db = dj_db / m                                
        
    return dj_db, dj_dw
```

ç”±æ­¤å¯å¾—ï¼Œå¤šå…ƒçº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
import numpy as np
import math, copy


def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): 
    """
    Performs batch gradient descent to learn theta. Updates theta by taking 
    num_iters gradient steps with learning rate alpha
    
    Args:
      X (ndarray (m,n))   : Data, m examples with n features
      y (ndarray (m,))    : target values
      w_in (ndarray (n,)) : initial model parameters  
      b_in (scalar)       : initial model parameter
      cost_function       : function to compute cost
      gradient_function   : function to compute the gradient
      alpha (float)       : Learning rate
      num_iters (int)     : number of iterations to run gradient descent
      
    Returns:
      w (ndarray (n,)) : Updated values of parameters 
      b (scalar)       : Updated value of parameter 
      """
    
    # An array to store cost J and w's at each iteration primarily for graphing later
    J_history = []
    w = copy.deepcopy(w_in)  #avoid modifying global w within function
    b = b_in
    
    for i in range(num_iters):

        # Calculate the gradient and update the parameters
        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None

        # Update Parameters using w, b, alpha and gradient
        w = w - alpha * dj_dw               ##None
        b = b - alpha * dj_db               ##None
      
        # Save cost J at each iteration
        if i<100000:      # prevent resource exhaustion 
            J_history.append( cost_function(X, y, w, b))

        # Print cost every at intervals 10 times or as many iterations if < 10
        if i% math.ceil(num_iters / 10) == 0:
            print(f"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   ")
        
    return w, b, J_history #return final w,b and J history for graphing

'''Example Inputs

# initialize parameters
initial_w = np.zeros_like(w_init)
initial_b = 0.
# some gradient descent settings
iterations = 1000
alpha = 5.0e-7
# run gradient descent 
w_final, b_final, J_hist = gradient_descent(X_train, y_train, initial_w, initial_b,
                                                    compute_cost, compute_gradient, 
                                                    alpha, iterations)
print(f"b,w found by gradient descent: {b_final:0.2f},{w_final} ")
m,_ = X_train.shape
for i in range(m):
    print(f"prediction: {np.dot(X_train[i], w_final) + b_final:0.2f}, target value: {y_train[i]}")
'''
```



#### 1.1.4.3	æ­£è§„æ–¹ç¨‹Normal Equation

**æ­£è§„æ–¹ç¨‹*Normal Equation***åŒæ ·æ˜¯è§£å†³çº¿æ€§å›å½’é—®é¢˜çš„å‡½æ•°ï¼Œå®ƒæœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

- **åªèƒ½ç”¨äºæ±‚è§£çº¿æ€§å›å½’é—®é¢˜**
- è§£å¾—$w,b$è€Œ**æ— éœ€è¿­ä»£**

ç¼ºç‚¹ï¼š

- **ä¸èƒ½æ¨å¹¿**åˆ°å…¶ä»–å­¦ä¹ ç®—æ³•
- å½“**ç‰¹å¾æ•°é‡å¾ˆå¤§**$(>10,000)$**æ—¶è®¡ç®—é€Ÿåº¦å¾ˆæ…¢**

æˆ‘ä»¬æ— éœ€çŸ¥é“æ­£è§„æ–¹ç¨‹å¦‚ä½•è¿ç®—ï¼Œåªéœ€çŸ¥é“å½“æˆ‘ä»¬åœ¨è°ƒç”¨æœºå™¨å­¦ä¹ åº“æ—¶ï¼Œ**åœ¨æ±‚è§£çº¿æ€§å›å½’é—®é¢˜æ—¶ï¼Œåº“å‡½æ•°æœ‰å¯èƒ½ä¼šä½¿ç”¨è¿™ç§æ–¹æ³•**

### 

### 1.1.5	ç‰¹å¾ç¼©æ”¾Feature Scaling

é¢å¯¹ç‰¹å¾æ•°é‡è¾ƒå¤šçš„æ—¶å€™ï¼Œä¿è¯è¿™äº›ç‰¹å¾å…·æœ‰ç›¸è¿‘çš„å°ºåº¦ï¼ˆæ— é‡çº²åŒ–ï¼‰ï¼Œå¯ä»¥ä½¿æ¢¯åº¦ä¸‹é™æ³•æ›´å¿«çš„æ”¶æ•›ã€‚è¿™ä¸¤å¼ å›¾ä»£è¡¨æ•°æ®æ˜¯å¦å‡ä¸€åŒ–çš„æœ€ä¼˜è§£å¯»è§£è¿‡ç¨‹ï¼ˆå·¦è¾¹æ˜¯æœªå½’ä¸€åŒ–çš„ï¼Œå³è¾¹å½’ä¸€åŒ–åï¼‰

<img src="Image/image-20220913213922489.png" alt="image-20220913213922489" style="zoom: 50%;" />

ä»è¿™ä¸¤å¼ å›¾å¯ä»¥çœ‹å‡ºï¼Œæ•°æ®å½’ä¸€åŒ–åï¼Œæœ€ä¼˜è§£çš„å¯»ä¼˜è¿‡ç¨‹æ˜æ˜¾ä¼šå˜å¾—å¹³ç¼“ï¼Œ**æ›´å®¹æ˜“æ­£ç¡®çš„æ”¶æ•›åˆ°æœ€ä¼˜è§£**

#### 1.1.5.1	å‡å€¼å½’ä¸€åŒ–Mean Normalization

**å‡å€¼å½’ä¸€åŒ–*Mean Normalization***æ˜¯ç‰¹å¾ç¼©æ”¾çš„ä¸€ç§æ–¹æ³•ï¼Œå…¶ç›®æ ‡æ˜¯ä½¿è®­ç»ƒæ ·ä¾‹ä¸­å„ç‰¹å¾å€¼åˆ†å¸ƒåœ¨[-1,1]ä¹‹é—´ï¼Œå…¶æ•°å­¦è¡¨è¾¾ä¸ºï¼š
$$
x = \frac{x-\mu}{max(x)-min(x)}
$$
å…¶ä¸­$x$æ˜¯ç‰¹å¾å€¼ï¼Œ$\mu$æ˜¯ç‰¹å¾å€¼$x$çš„å¹³å‡å€¼ï¼Œ$max(x),min(x)$åˆ†åˆ«æ˜¯ç‰¹å¾å€¼$x$ä¸­çš„æœ€å¤§ã€æœ€å°å€¼

#### 1.1.5.2	Z-scoreæ ‡å‡†åŒ–Z-score Normalization

***Z-score*æ ‡å‡†åŒ–**éœ€è¦è®¡ç®—å„ç‰¹å¾å€¼çš„**æ ‡å‡†å·®*Standard Deviation***ï¼Œç”¨ç¬¦å·$\sigma$è¡¨ç¤ºï¼Œå…¶æ•°å­¦è¡¨è¾¾ä¸º
$$
x = \frac{x - \mu}{\sigma}
$$


### 1.1.6	åˆ¤æ–­æ¢¯åº¦ä¸‹é™æ˜¯å¦æ”¶æ•›

æ¢¯åº¦ä¸‹é™ç®—æ³•çš„ç›®æ ‡æ˜¯æ‰¾åˆ°æœ€å°æˆæœ¬å‡½æ•°ï¼Œå› æ­¤åˆ¤æ–­æ¢¯åº¦ä¸‹é™æ˜¯å¦æ”¶æ•›ï¼Œå°±è¦çœ‹æˆæœ¬å‡½æ•°æ˜¯å¦éšæ¢¯åº¦ä¸‹é™çš„è¿­ä»£æ¬¡æ•°å¢åŠ è€Œå‡å°‘ã€‚

- ç›®æµ‹ï¼šæˆ‘ä»¬å¯æ ¹æ®è¿­ä»£æ¬¡æ•°å’Œæˆæœ¬å‡½æ•°å˜åŒ–ç»˜åˆ¶å·¦å›¾ï¼Œå¯çœ‹åˆ°å½“è¿­ä»£300æ¬¡æ—¶å·²åŸºæœ¬å¹³è¡¡ï¼Œè¿­ä»£400æ¬¡æ—¶æ›²çº¿è¿‘ä¼¼æ°´å¹³çº¿ï¼Œæ­¤æ—¶å¯è®¤ä¸ºæ¢¯åº¦ä¸‹é™æ”¶æ•›
- **è‡ªåŠ¨æ”¶æ•›æµ‹è¯•*Automatic Convergence Test***ï¼šæˆ‘ä»¬å¯å¼•å…¥$\varepsilon = 10^{-3}$ï¼Œå½“$J_2-J_1 = \Delta J(\vec{w},b) \leq \varepsilon$æ—¶ï¼Œå¯è®¤ä¸ºæ”¶æ•›

![image-20220913215938004](Image/image-20220913215938004.png)

é€šè¿‡åˆ¤æ–­æ¢¯åº¦ä¸‹é™æ˜¯å¦æ”¶æ•›å’Œè¿­ä»£æ¬¡æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹å­¦ä¹ ç‡$\alpha$è¿›è¡Œè°ƒæ•´



### 1.1.7	ç‰¹å¾å·¥ç¨‹Feature Engineering

**ç‰¹å¾å·¥ç¨‹*Feature Engineering***ï¼Œå³æ ¹æ®ç›´è§‰ï¼Œå€ŸåŠ©è½¬æ¢æˆ–ç»„åˆåŸæœ‰ç‰¹å¾çš„æ–¹å¼ï¼Œè®¾è®¡æ–°çš„ç‰¹å¾

![image-20220913221310174](Image/image-20220913221310174.png)

ä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œå‡è®¾è¦é¢„æµ‹æˆ¿å±‹ä»·æ ¼ï¼Œç°æœ‰ç‰¹å¾ä¸ºæˆ¿å±‹ä¸´è¡—é•¿åº¦(frontage)å’Œæˆ¿å±‹çºµæ·±(depth)ï¼Œè®°ä½œ$x_1,x_2$

åˆ™æ­¤æ—¶æˆ‘ä»¬çš„çº¿æ€§å›å½’æ–¹ç¨‹$f_{\vec{w},b}(\vec{x})=w_1x_1+w_2x_2+b$

ä½†æˆ‘ä»¬å‘ç°ï¼Œæˆ¿å±‹é¢ç§¯æ¯”æˆ¿å±‹ä¸´è¡—é•¿åº¦å’Œçºµæ·±å•ç‹¬æ‹¿å‡ºæ¥ï¼Œæ›´èƒ½ä½“ç°æˆ¿å±‹ä»·å€¼ï¼Œå› æ­¤æˆ‘ä»¬å¼•å…¥æ–°çš„ç‰¹å¾æˆ¿å±‹é¢ç§¯$x_3$ï¼Œ

$x_3=x_1x_2$ï¼Œæ­¤æ—¶æ–°çš„ç‰¹å¾æ–¹ç¨‹ä¸º$f_{\vec{w},b}(\vec{x})=w_1x_1+w_2x_2+w_3x_3+b$

### 1.1.8	å¤šé¡¹å¼å›å½’Polynomial Regression

æˆ‘ä»¬åœ¨å¤šå…ƒçº¿æ€§å›å½’å’Œç‰¹å¾å·¥ç¨‹çš„åŸºç¡€ä¸Šå¼•å…¥**å¤šé¡¹å¼å›å½’*Polynomial Regression***ï¼Œä»¥æ›´å¥½åœ°è¡¨ç¤ºéçº¿æ€§å‡½æ•°



## 1.2	åˆ†ç±»Classification

åˆ†ç±»æ˜¯å¦ä¸€ç§ç›‘ç£å­¦ä¹ çš„ç®—æ³•ï¼Œå…¶ä¸­å¯¹äºåªæœ‰ä¸¤ç§å¯èƒ½è¾“å‡ºçš„åˆ†ç±»é—®é¢˜ç§°ä¸º**äºŒå…ƒåˆ†ç±»*Binary Classification***

åœ¨äºŒå…ƒåˆ†ç±»é—®é¢˜ä¸­ï¼Œåˆ†ç±»ç»“æœåªæœ‰0å’Œ1ä¸¤ç§ï¼Œå³$P(y=0)+P(y=1)=1$ï¼Œå…¶ä¸­$y$æ˜¯åˆ†ç±»ç»“æœ

è¦è§£å†³äºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œçº¿æ€§å›å½’æ˜¾ç„¶ä¸é€‚ç”¨ï¼Œä¸ºæ­¤æˆ‘ä»¬æå‡ºäº†**é€»è¾‘å›å½’*Logistic Regression***ï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè™½ç„¶é€»è¾‘å›å½’ä¸­æœ‰â€œå›å½’â€ä¸€è¯ï¼Œä½†å®ƒä»æ˜¯**ä¸€ç§åˆ†ç±»é—®é¢˜çš„è§£å†³æ–¹æ¡ˆ**

ä¸ºäº†æ„å»ºé€»è¾‘å›å½’ç®—æ³•ï¼Œæˆ‘ä»¬è¿˜éœ€è¦å¼•å…¥**é€»è¾‘å‡½æ•°*Sigmoid Function/Logistic Function***ï¼Œå…¶ç‰¹ç‚¹æ˜¯**è¾“å‡ºåªä»‹äº(0,1)ä¹‹é—´**

<img src="Image/image-20220914153105031.png" alt="image-20220914153105031" style="zoom:67%;" />

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦æ„é€ çš„é€»è¾‘å‡½æ•°å½¢å¼å¦‚ä¸‹ï¼š
$$
f_{\vec{w},b}(\vec{x}) = g(z) = g(\vec{w}\cdot\vec{x}+b) = \frac{1}{1+e^{-(\vec{w}\cdot\vec{x}+b)}}
$$
å…¶**å‡½æ•°å€¼å«ä¹‰**ä¸ºï¼š**åˆ†ç±»ç»“æœä¸º1çš„å¯èƒ½æ€§**ï¼Œå³å½“$f_{\vec{w},b}(\vec{x}) = 0.7$æ—¶è¡¨ç¤ºï¼Œæœ‰70%å¯èƒ½åˆ†ç±»ç»“æœä¸º1

é€»è¾‘å›å½’ä¸­çš„é€»è¾‘å‡½æ•°è¿˜æœ‰å…¶ä»–è¡¨ç°å½¢å¼ï¼Œå¦‚ï¼š
$$
f_{\vec{w},b}(\vec{x}) = P(y=1|\vec{x};\vec{w},b)
$$
è¯¥å‡½æ•°å€¼å«ä¹‰æ˜¯ç»™å®šè¾“å…¥$\vec{x}$ï¼Œå‚æ•°$\vec{w},b$ï¼Œåˆ†ç±»ç»“æœä¸º1çš„å¯èƒ½

### 1.2.1	å†³ç­–è¾¹ç•ŒDecision Boundary

è¦é€šè¿‡é€»è¾‘å‡½æ•°è§£å†³é€»è¾‘å›å½’é—®é¢˜ï¼Œæˆ‘ä»¬é¦–å…ˆè¦ç¡®å®šä¸€ä¸ªé˜ˆå€¼ï¼Œå½“è¶…è¿‡æˆ–å°äºè¿™ä¸ªé˜ˆå€¼æ—¶ï¼Œè¾“å‡ºåˆ†ç±»ç»“æœä¸º0æˆ–1ï¼Œè€Œè¿™ä¸ªé˜ˆå€¼å°±è¢«ç§°ä¸º**å†³ç­–è¾¹ç•Œ*Decision Boundary***ï¼Œå› ä¸ºåœ¨å†³ç­–è¾¹ç•Œä¸Šï¼Œæˆ‘ä»¬å¯¹è¾“å‡º0æˆ–1çš„æ€åº¦æ˜¯ä¸­ç«‹çš„

<img src="Image/image-20220914155114332.png" alt="image-20220914155114332" style="zoom:50%;" />

å¦‚å›¾æ‰€ç¤ºï¼Œå‡å®šæœ‰æ•°æ®é›†å¦‚ä¸‹ï¼Œè“åœˆè¡¨ç¤ºåˆ†ç±»ç»“æœä¸º0ï¼Œçº¢å‰è¡¨ç¤ºåˆ†ç±»ç»“æœä¸º1ï¼Œç”±äºæœ‰ä¸¤ç‰¹å¾é‡$x_1,x_2$ï¼Œå› æ­¤æˆ‘ä»¬çš„é€»è¾‘å‡½æ•°$f_{\vec{w},b}(\vec{x}) = g(z) = g(\vec{w}\cdot\vec{x}+b) = g(w_1x_1+w_2x_2+b)$

è¦æ‰¾åˆ°å†³ç­–è¾¹ç•Œï¼Œå³æ»¡è¶³$z=0$ï¼Œä»¤$z = \vec{w}\cdot\vec{x}+b = 0 = w_1x_1+w_2x_2+b=0$

å‡è®¾$w_1=1,w_2=1,b=-3$ï¼Œåˆ™æœ‰$z=x_1+x_2-3=0$ï¼Œå¯çŸ¥å†³ç­–è¾¹ç•Œä¸º$x_1+x_2=3$ï¼Œå³å¦‚å›¾æ‰€ç¤ºç´«è‰²ç›´çº¿

### 1.2.2	é€»è¾‘å›å½’ä¸­çš„æˆæœ¬å‡½æ•°

è®¾$m$ä¸ºè®­ç»ƒæ ·æœ¬æ•°ï¼Œ$n$ä¸ºç‰¹å¾å€¼æ•°ï¼Œç›®æ ‡å€¼$y$å–å€¼åªèƒ½ä¸º0æˆ–1

åœ¨çº¿æ€§å›å½’é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨å¹³æ–¹è¯¯å·®å‡½æ•°æ¥è¯„ä¼°æ‹Ÿåˆç¨‹åº¦ï¼Œä½†åœ¨é€»è¾‘å›å½’é—®é¢˜ä¸­ï¼Œå¹³æ–¹è¯¯å·®å‡½æ•°å¾—åˆ°çš„ç»“æœå¹¶ä¸æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬å¾ˆéš¾æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ï¼š

<img src="Image/image-20220914160705651.png" alt="image-20220914160705651" style="zoom:50%;" />

#### 1.2.2.1	æŸå¤±å‡½æ•°Loss Function

æˆ‘ä»¬çš„ä»£ä»·å‡½æ•°$J(\vec{w},b)= \frac{1}{m}\sum_{i=1}^{m}\frac{1}{2}(f_{\vec{w},b}(\vec{x}^{(i)}-y^{(i)}))^2 = \frac{1}{m}\sum_{i=1}^{m} L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})$

æˆ‘ä»¬å°†$L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})$ç§°ä¸º**æŸå¤±å‡½æ•°*Loss Function***ï¼Œ**æŸå¤±å‡½æ•°è¡¨ç¤ºé€»è¾‘å‡½æ•°åœ¨è¯¥è®­ç»ƒæ ·ä¾‹(è¿™ä¸€ç‚¹)ä¸Šçš„é¢„æµ‹ä»£ä»·ï¼Œè€Œä»£ä»·å‡½æ•°è¡¨ç¤ºå‡½æ•°åœ¨å…¨éƒ¨è®­ç»ƒæ ·ä¾‹ä¸Šçš„å¹³å‡ä»£ä»·**ã€‚

æŸå¤±å‡½æ•°åœ¨é€»è¾‘å›å½’é—®é¢˜çš„æ•°å­¦è¡¨è¾¾å¦‚ä¸‹ï¼š
$$
L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) =

\begin{cases}

-log(f_{\vec{w},b}(\vec{x}^{(i)}))       & y^{(i)}=1 \\

-log(1-f_{\vec{w},b}(\vec{x}^{(i)}))   & y^{(i)}=0 \\

\end{cases}
$$
![image-20220914162609175](Image/image-20220914162609175.png)

- **å½“é¢„æµ‹ç»“æœä¸º1æ—¶**ï¼ŒæŸå¤±å‡½æ•°æ›²çº¿å¦‚å›¾ä¸­å³åæ ‡ç³»è“è‰²æ›²çº¿æ‰€ç¤ºï¼Œç”±äºé€»è¾‘å‡½æ•°å–å€¼æ€»ä»‹äº(0,1)ï¼Œå› æ­¤å…¶æ›²çº¿åªå–ç¬¬ä¸€è±¡é™ï¼Œå¦‚ç´«æ¡†æ‰€ç¤º

  å°†ç´«æ¡†ä¸­æ›²çº¿æ”¾å¤§ä¸ºå·¦ä¾§åæ ‡ç³»ï¼Œæ¨ªè½´ä¸ºé€»è¾‘å‡½æ•°å–å€¼ï¼Œçºµè½´ä¸ºæŸå¤±å‡½æ•°ï¼Œæ ¹æ®æ›²çº¿å¯ä»¥çœ‹å‡ºï¼Œ**å½“é€»è¾‘å‡½æ•°é¢„æµ‹ç»“æœ(å³é€»è¾‘å‡½æ•°å€¼)è¶Šæ¥è¿‘1æ—¶ï¼ŒæŸå¤±è¶Šå°**

  ![image-20220914163212310](Image/image-20220914163212310.png)

- **å½“é¢„æµ‹ç»“æœä¸º0æ—¶**ï¼ŒæŸå¤±å‡½æ•°æ›²çº¿å¦‚å›¾æ‰€ç¤ºï¼Œ**å½“é€»è¾‘å‡½æ•°é¢„æµ‹ç»“æœ(å‡½æ•°å€¼)è¶Šæ¥è¿‘0æ—¶ï¼ŒæŸå¤±è¶Šå°**

#### 1.2.2.2	ç®€åŒ–ä»£ä»·å‡½æ•°

é¦–å…ˆç®€åŒ–æŸå¤±å‡½æ•°ï¼š
$$
L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)}) =

\begin{cases}

-log(f_{\vec{w},b}(\vec{x}^{(i)}))       & y^{(i)}=1 \\

-log(1-f_{\vec{w},b}(\vec{x}^{(i)}))   & y^{(i)}=0 \\

\end{cases}

\\ = -y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) - (1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)}))
$$
å¯å¾—ä»£ä»·å‡½æ•°ï¼š
$$
J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}[L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})]
=-\frac{1}{m}\sum_{i=1}^{m}\left[ y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) + (1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})) \right]
$$

### 1.2.3	å®ç°æ¢¯åº¦ä¸‹é™

é€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™å‡½æ•°ä¸çº¿æ€§å›å½’ç›¸åŒï¼Œä½†ç”±äºå®ƒä»¬å®šä¹‰çš„$f$ä¸åŒï¼Œå› æ­¤å¹¶ä¸æ˜¯åŒä¸€ç±»é—®é¢˜

ä½†ç±»ä¼¼äºçº¿æ€§å›å½’ï¼Œé€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™é—®é¢˜åŒæ ·æœ‰ä»¥ä¸‹å‡ ä¸ªå…±åŒç‚¹ï¼š

- å…³æ³¨æ¢¯åº¦ä¸‹é™çš„å­¦ä¹ æ›²çº¿ï¼Œè°ƒæ•´å­¦ä¹ ç‡$\alpha$
- çŸ¢é‡åŒ–ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦
- ç‰¹å¾ç¼©æ”¾ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦



### 1.2.4	è¿‡æ‹ŸåˆOverfittingä¸æ¬ æ‹Ÿåˆunderfitting

ä»¥çº¿æ€§å›å½’ä¸ºä¾‹ï¼š

<img src="Image/image-20220914194540953.png" alt="image-20220914194540953" style="zoom: 50%;" />

- å½“è®­ç»ƒæ¨¡å‹ä¸èƒ½å¾ˆå¥½åœ°æ‹Ÿåˆè®­ç»ƒæ ·æœ¬é›†æ—¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º**æ¬ æ‹Ÿåˆ*Underfitting***æˆ–**é«˜åå·®*High Bias***
- å½“è®­ç»ƒæ¨¡å‹èƒ½å¾ˆå¥½æ‹Ÿåˆè®­ç»ƒæ ·æœ¬ï¼Œä¸”æ¨¡å‹å¯¹æ–°çš„è®­ç»ƒæ ·æœ¬çš„é¢„æµ‹å€¼ä¸ç›®æ ‡å€¼ååˆ†æ¥è¿‘æ—¶ï¼Œæˆ‘ä»¬è®¤ä¸ºæ¨¡å‹å®ç°äº†**æ³›åŒ–*Generalization***
- å½“è®­ç»ƒæ¨¡å‹è¿‡äºæ‹Ÿåˆè®­ç»ƒæ ·æœ¬ï¼Œå³ä¸ºäº†æ‹Ÿåˆæ ·æœ¬è€Œäº§ç”Ÿäº†ä¸åˆå¸¸ç†çš„å‡½æ•°(å¦‚æ›²çº¿æ‘‡æ‘†ä¸å®š)æˆ–æ¨¡å‹æ— æ³•æ¨å¹¿æ—¶ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º**è¿‡æ‹Ÿåˆ*Overfitting***æˆ–**é«˜æ–¹å·®*High Variance***ï¼Œå¦‚åæ ‡ç³»3æ‰€ç¤ºï¼Œåœ¨ç´«è‰²è™šçº¿æ‰€æŒ‡ç¤ºç‚¹å¤„ï¼Œé¢„æµ‹å€¼æ˜æ˜¾ä¸åˆç†

æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆé—®é¢˜åœ¨åˆ†ç±»é—®é¢˜ä¸­åŒæ ·ä¼šå‡ºç°ï¼š

![image-20220914195902469](Image/image-20220914195902469.png)



#### 1.2.4.1	è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜

- æ”¶é›†æ›´å¤šè®­ç»ƒæ ·æœ¬ï¼Œé‡æ–°è®­ç»ƒæ¨¡å‹
- **ç‰¹å¾é€‰æ‹©*Future Selection***ï¼šæ ¹æ®æ ·æœ¬ç‰¹ç‚¹å’Œæ ·æœ¬æ•°é‡ï¼Œé€‰æ‹©åˆé€‚çš„ç‰¹å¾ï¼Œè‹¥æ•°æ®é›†è¾ƒå°ï¼Œåˆ™ç‰¹å¾ä¸åº”è¿‡å¤š
- **æ­£åˆ™åŒ–*Regularization***ï¼šæ­£åˆ™åŒ–é¼“åŠ±ç®—æ³•ç¼©å°å‚æ•°$w$è€Œæ— éœ€æ°å¥½è®¾ç½®ä¸º0ï¼ˆä¸€ç§æ¸©å’Œçš„å‡å°‘ç‰¹å¾å€¼çš„æ–¹æ³•ï¼‰

####  1.2.4.2	æ­£åˆ™åŒ–Regularization

æ­£åˆ™åŒ–çš„ç›®çš„æ˜¯ç®€åŒ–æ¨¡å‹ï¼Œé€šè¿‡å°½å¯èƒ½ç¼©å°å‚æ•°çš„æ–¹å¼ï¼Œä»¥ä¸€ç§æ¸©å’Œçš„æ–¹å¼ç¼©å°å‚æ•°å¯¹æ¨¡å‹çš„å½±å“

æ­£åˆ™åŒ–çš„å®ç°æ–¹å¼æ˜¯æƒ©ç½šæ‰€æœ‰ç‰¹å¾ï¼Œå³æ‰€æœ‰$w$å‚æ•°ï¼Œå…¶æ•°å­¦è¡¨è¾¾å¦‚ä¸‹ï¼š
$$
J(\vec{w},b)=\frac{1}{2m}\sum_{i=1}^{m}(f_{\vec{w},b}(\vec{x}^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2
$$
å…¶ä¸­$\lambda$è¢«ç§°ä¸ºæ­£åˆ™åŒ–å‚æ•°ï¼Œè€Œ$\frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2$è¢«ç§°ä¸ºæ­£åˆ™åŒ–é¡¹

ç°åœ¨æˆ‘ä»¬çš„æˆæœ¬å‡½æ•°åˆ†ä¸ºä¸¤é¡¹ï¼šå‡æ–¹è¯¯å·®æˆæœ¬å’Œæ­£åˆ™åŒ–é¡¹

å½“$\lambda$è¿‡å¤§æ—¶ï¼Œæ‰€æœ‰å‚æ•°$w$éƒ½å°†è¶‹è¿‘äº0ï¼›å½“$\lambda$å€¼è¿‡å°æ—¶ï¼Œè¿‡æ‹Ÿåˆé—®é¢˜ä»å¾—ä¸åˆ°è§£å†³

ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©åˆé€‚çš„æ­£åˆ™åŒ–å‚æ•°$\lambda$ï¼Œæ¥è§£å†³è¿‡æ‹Ÿåˆé—®é¢˜ã€‚



##### 1.2.4.2.1	çº¿æ€§å›å½’çš„æ­£åˆ™æ–¹æ³•

åœ¨ä¹‹å‰çš„çº¿æ€§å›å½’å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°æ¢¯åº¦ä¸‹é™ç®—æ³•ä¸ºï¼š
$$
w_j\ =\ w_j-\alpha \frac{\partial{}}{\partial{w_j}}J(\vec{w},b)\\
b\ = \ b-\alpha \frac{\partial{}}{\partial{b}}J(w,b)
$$
ç°åœ¨å¼•å…¥æ­£åˆ™åŒ–åçš„æˆæœ¬å‡½æ•°å¹¶å±•å¼€ï¼Œå¾—ï¼š
$$
w_j\ =\ w_j-\alpha \frac{\partial{}}{\partial{w_j}}J(\vec{w},b) = w_j-\alpha (\frac{1}{m}\sum_{i=1}^m(ğ‘“_{\vec{w},ğ‘}(ğ‘¥^{(i)})-y^{(i)})x^{(i)} + \frac{\lambda}{m}w_j)
\\
b\ = \ b-\alpha \frac{\partial{}}{\partial{b}}J(\vec{w},b) = b-\alpha (\frac{1}{m}\sum_{i=1}^m(ğ‘“_{\vec{w},ğ‘}(ğ‘¥^{(i)})-y^{(i)})
$$
ç”±äºæˆ‘ä»¬åªå¯¹å‚æ•°$w$è¿›è¡Œæ­£åˆ™åŒ–ï¼Œå› æ­¤$b$æ— éœ€æ·»åŠ æ­£åˆ™åŒ–é¡¹

è°ƒæ•´é¡¹é¡ºåºå¯å¾—ï¼š
$$
w_j\ =\ w_j-\alpha \frac{\partial{}}{\partial{w_j}}J(\vec{w},b)
\\
= w_j-\alpha (\frac{1}{m}\sum_{i=1}^m(ğ‘“_{\vec{w},ğ‘}(ğ‘¥^{(i)})-y^{(i)})x^{(i)} + \frac{\lambda}{m}w_j)
\\
=w_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(ğ‘“_{\vec{w},ğ‘}(ğ‘¥^{(i)})-y^{(i)})x^{(i)}
$$
è¿™æ ·çš„å½¢å¼å¯ä»¥æ›´ç›´è§‚çš„çœ‹å‡ºï¼Œ$\lambda$æ˜¯å¯¹å‚æ•°$w_j$çš„æƒ©ç½š(Shrink $w_j$)



##### 1.2.4.2.2	é€»è¾‘å›å½’çš„æ­£åˆ™æ–¹æ³•

åŒä¸Šï¼Œæ›´æ–°ä»£ä»·å‡½æ•°ï¼š
$$
J(\vec{w},b)=\frac{1}{m}\sum_{i=1}^{m}[L(f_{\vec{w},b}(\vec{x}^{(i)}),y^{(i)})] + \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2
\\
=-\frac{1}{m}\sum_{i=1}^{m}\left[ y^{(i)}log(f_{\vec{w},b}(\vec{x}^{(i)})) + (1-y^{(i)})log(1-f_{\vec{w},b}(\vec{x}^{(i)})) \right]  + \frac{\lambda}{2m}\sum_{j=1}^{n}w_j^2
$$
æ›´æ–°æ¢¯åº¦ä¸‹é™å‡½æ•°ï¼š
$$
w_j\ =\ w_j-\alpha \frac{\partial{}}{\partial{w_j}}J(\vec{w},b) = w_j-\alpha (\frac{1}{m}\sum_{i=1}^m(ğ‘“_{\vec{w},ğ‘}(ğ‘¥^{(i)})-y^{(i)})x^{(i)} + \frac{\lambda}{m}w_j)
\\
b\ = \ b-\alpha \frac{\partial{}}{\partial{b}}J(\vec{w},b) = b-\alpha (\frac{1}{m}\sum_{i=1}^m(ğ‘“_{\vec{w},ğ‘}(ğ‘¥^{(i)})-y^{(i)})
$$
äº‹å®ä¸Šï¼Œè¿™ä¸çº¿æ€§å›å½’çš„æ­£åˆ™æ–¹æ³•å½¢å¼å®Œå…¨ç›¸åŒ


