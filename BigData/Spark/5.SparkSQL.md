# 5.	Spark SQL

## 5.0	Spark SQL简介

1.什么是SparkSQL？

SparkSQL是Spark用于处理海量**结构化数据**的模块



2.为什么要使用SparkSQL？

- SparkSQL本身优秀：支持SQL、性能强、自动优化、API简单、兼容HIVE
- 企业常用SparkSQL处理业务数据：离线开发、数仓搭建、科学计算、数据分析



3.SparkSQL的特点？

- 融合性：SQL可以无缝集成于代码，随时使用SQL处理数据
- 统一数据访问：一套标准API可以读写不同数据源
- 兼容HIVE：可以使用SparkSQL直接计算，并生成HIVE数据表
- 标准化连接：支持标准化JDBC/ODBC连接，方便与各种数据库进行数据交互



4.SparkSQL和HIVE的异同：

SparkSQL与Hive都是分布式SQL计算引擎，用于处理大规模结构化数据，但SparkSQL有更好的性能

<img src="Image/image-20230808165731122.png" alt="image-20230808165731122" style="zoom: 50%;" />



5.SparkSQL的数据抽象

 <img src="Image/image-20230808170450384.png" alt="image-20230808170450384" style="zoom: 50%;" />

Pandas使用DataFrame作为数据抽象，其内部实现是二维表数据结构，适合使用SQL、适合处理结构化数据，但它是本地数据类型，因此不能用于处理分布式数据。

SparkCore使用RDD作为数据抽象，其虽然数据集合是分布式，但没有标准数据结构，因此不适合处理结构化数据。

SparkSQL使用DataFrame作为数据抽象，内部实现是二维表数据结构，且使用分布式集合，因此适合使用SQL和处理分布式结构化数据。



6.SparkSQL在不同语言下的数据类型

SparkSQL数据抽象对象：

- DataSet，实际上是泛型DataFrame，可用于Java、Scala
- DataFrame，可用于Java、Scal、Python



7.DataFrame与RDD的异同

![image-20230808171129101](Image/image-20230808171129101.png)

![image-20230808171303333](Image/image-20230808171303333.png)

## 5.1	SparkSession

在RDD编程时，Spark程序的执行入口对象是SparkContext

在Spark2.0后，Spark使用SparkSession作为Spark编程的统一入口对象，可以同时用于SparkSQL编程和SparkCore编程。



```python
from pyspark.sql import SparkSession


if __name__ == '__main__':
    # 以构建器模式构造SparkSession对象
    spark = SparkSession.builder\
    		.appName('local[*]')\
    		.master('local[*]')\
    		.config('spark,sql.shuffle.partitions', '4')\
    		.getOrCreate()
    # appName()方法设置Application名, config()设置属性
    # getOrCreate()方法创建SparkSession对象
    
    # 通过SparkSession对象获取SparkContext对象
    sc = spark.sparkContext
    
    # SparkSQL HelloWorld
    df = spark.read.csv('./data/stu_score.txt', seq=',', header=False)
    df2 = df.toDF('id', 'name', 'score')
    df2.printSchema()	# 查看表结构
    df2.show()	# 查看表内数据
    
    # SQL风格编程
    df2.createTempView('score')
    spark.sql('select * from score where name="语文" LIMIT 5').show()
    
    # API模式开发(DSL风格)
    df2.where('name="语文"').limit(5).show()
```



## 5.2	DataFrame

### 5.2.1	DataFrame的组成

DataFrame是一个二维表结构，因此具有三个点：行、列、表结构描述

如在MySQL中的一张表：

- 由许多行组成
- 数据也分为多个列
- 表结构信息：列、列名、列数据类型、列约束等

基于这个前提，DataFrame组成如下：

- 结构层面：
  - StructType对象描述整个DataFrame表结构
  - StructField对象描述一个列的信息
- 数据层面：
  - Row对象记录一行数据
  - Column对象记录一列数据，并包含列信息

![image-20230808173152755](Image/image-20230808173152755.png)



### 5.2.2	基于RDD构造DataFrame

DataFrame与RDD都是分布式数据集，因此只需转换其内部存储结构为二维表结构，就可以将RDD转换为DataFrame：

```python
rdd = sc.textFile('./data/sql/people.txt')\
		.map(lambda x: x.split(','))\
		.map(lambda x: [x[0], int(x[1])])
# DataFrame中列名从RDD中推断, 因此需要在RDD中做类型转换
# 要将RDD转换为二维表结构, 以转换为DataFrame

# 这里只传入列名，是否为空默认为True
df = spark.createDataFrame(rdd, schema=['name', 'age'])

# 查看DataFrame表结构
df.printSchema()

# 输出DataFrame数据
# 参数1为输出行数, 默认为20; 
# 参数2表示是否对列截断, 若列数据长度超过20个字符, 后续内容不显示以...代替, 默认True
df.show(20, False)

# 注册成临时视图, 用于SQL查询
df.createOrReplaceTempView('people')
spark.sql('select * from people where age < 30').show()
```



除了先在RDD中转换数据格式，再直接将RDD转换为DataFrame，还有更好的方法，用StructType描述表结构，然后用RDD对象和StructType对象共同构造DataFrame对象：

```python
from pySpark.sql.types import StructType, StringType, IntegerType

rdd = sc.textFile('./data/sql/people.txt')\
		.map(lambda x: x.split(','))\
		.map(lambda x: [x[0], int(x[1])])

# 构造表结构的描述对象StructType
schema = StructType()\
	.add('name', StringType(), nullable=True)\
	.add('age', IntegerType(), nullable=False)
# 基于StructType, 实现RDD到DataFrame的转换
df = spark.createDataFrame(rdd, schema=schema)
```



除此之外，RDD还可以调用.toDF()方法，直接转换为DataFrame：

```python
from pySpark.sql.types import StructType, StringType, IntegerType

rdd = sc.textFile('./data/sql/people.txt')\
		.map(lambda x: x.split(','))\
		.map(lambda x: [x[0], int(x[1])])

# 构造表结构的描述对象StructType
schema = StructType()\
	.add('name', StringType(), nullable=True)\
	.add('age', IntegerType(), nullable=False)

# 只传列名，类型靠推断, 是否为空默认True
df = rdd.toDF(['name', 'age'])

# 基于StructType, 调用.toDF()方法, 实现RDD到DataFrame的转换
df = rdd.toDF(schema)
```



### 5.2.3	基于Pandas构造DataFrame

由于Pandas和SparkSQL的DataFrame都是二维表结构，因此只需要将本地集合转换为分布式集合即可。

```python
# 构造Pandas的DataFrame
pd_df = pd.DataFrame({
    'id': [1, 2, 3],
    'name': ['张三', '李四', '王五'],
    'age': [11, 12, 13]
})

# 将Pandas的DataFrame转换为SparkSQL的DataFrame
spark_df = spark.createDataFrame(pd_df)
```



###  5.2.4	DataFrame读取外部数据

通过使用SparkSQL的统一API进行数据读取，构造DataFrame：

```python
sparkSession.read\
			.foramt('text|csv|json|parquet|orc|avro|jdbc|...')\	# 支持多种数据源
			.option('K', 'V')\	# option可选, 如传入数据库的账号密码
			.schema(StructType | str)\	# str语法如.shcema('name STRING', 'age INT')
			.load('被读取文件路径, 支持本地文件系统和HDFS')
```

读取text数据源：

```python
# 数据特点是一整行作为一列读取, 默认列名为value, 类型为str
schema = StructType().add('data', StringType())
df = spark.read\
    .foramt('text')\
    .schema(schema)\
    .load('./data/people.txt')
```

读取JSON数据源：

```python
# JSON数据源自带类型信息
df = spark.read.format('json').load('./data/people.json')
```

读取CSV数据源：

```python
df = spark.read.format('csv')\
	.option('sep', ',')\	# 设置分隔符
	.option('header', False)\	# 无表头
	.option('encoding', 'utf-8')\	# 设置编码格式
	.schema('name STRING, age INT, job STRING')\	# 设置列名和类型, 默认自动生成列名和str类型
    .load('./data/people.csv')
```

读取parquet数据：

> parquet是Spark常用的**列式存储文件格式**，与Hive的ORC格式类似。
>
> parquet与普通文本文件区别：
>
> - 内置schema（列名\列类型\是否允许为空）
> - 按列存储
> - 序列化存储（可压缩，体积小）

```python
df = spark.read.format('parquet').load('./data/users.parquet')
```



## 5.3	DataFrame编程

### 5.3.1	DSL和SQL编程风格

DataFrame支持两种编程风格：

- DSL风格（领域特定语言，即DataFrame特有API）：调用API方式处理数据，如df.where().litmit()

> ```python
> # 获取Column对象
> id_column = df['id']
> subject_column = df['subject']
> 
> # DLS风格select, 三种方法效果一样
> df.select(['id', 'subject']).show()
> df.select('id', 'subject').show()
> df.select(id_column, subject_column).show()
> 
> # filter API
> df.filter('score < 99').show()
> df.filter(df['score'] < 99).show()
> 
> # where API, 与filter API相同
> df.where('score < 99').show()
> df.where(df['score'] < 99).show()
> 
> # groupBy API 
> df.groupBy('subject').count().show()
> df.groupBy(df['subject']).count().show()
> # 其他API返回的是DataFrame, 而groupBy返回的是GroupedData
> # GroupedData是由分组关系的数据结构，类似于SQL分组后的数据结构, 后接聚合: sum/avg/count/min/max, 聚合后返回的是DataFrame
> ```

- SQL风格：使用SQL语句处理DataFrame数据：spark.sql('select * from xxx*')

> DataFrame可以被看做关系型数据表，使用spark.sql()执行SQL语句，结果返回DataFrame
>
> ```python
> # 使用SQL风格语法前，先将DataFrame注册为表：
> df.createTempView('score')	# 注册临时视图
> df.createOrReplaceTempView('score_2')	# 注册临时视图, 若存在则替换
> df.createGlobalTempView('score_3')	# 注册全局视图
> # 全局指的是在一个程序内的多个SparkSession对象中都可以调用, 查询前要带前缀"global_temp."
> # 临时只在当前SparkSession中可用
> 
> # 通过SparkSession对象的SQL API执行SQL语句
> spark.sql('select subject, count(*) as cnt from score group by subject').show()
> spark.sql('select subject, count(*) as cnt from score_2 group by subject').show()
> spark.sql('select subject, count(*) as cnt from global_temp.score_3 group by subject').show()
> ```



### 5.3.2	SparkSQL库

PySpark提供了pyspark.sql.functions包，提供了一系列SparkSQL的计算函数，这些函数返回值大多为Column对象，如：

```python
from pyspark.sql import functions as F

new_column = F.split(old_column, ' ')
```



### 5.3.3	词频统计案例练习

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F


if __name__ == '__main__':
    spark = SparkSession.builder \
    					.appName('test') \
    					.master('local[*]') \
    					.getOrCreate()
    # SQL风格
    sc = spark.sparkContext
    rdd = sc.textFile('./data/word.txt') \
    		.flatMap(lambda x: x.split(' ')) \
    		.map(lambda x: [x])
    df = rdd.toDF(['word'])
    
    df.createTempView('words')	# 注册为临时视图
    spark.sql('SELECT word, COUNT(*) AS cnt FROM words GROUP BY word ORDER BY cnt DESC').show()
    
    # DSL风格
    df = spark.read.format('text').load('./data/word.txt')
    # withColumn: 对已存在的列操作，返回新的列，若名字与旧列相同则替换，否则作为新列存在
    # explode: 将一行转换为多列
    df = df.withColumn('value', F.explode(F.split(df['value'], ' '))) \
    		.groupBy('value') \
    		.count() \
    		.withColumnRenamed('value', 'word') \
    		.withColumnRenamed('count', 'cnt') \
    		.orderBy('cnt', ascending=False) \
    		.show()
```



###  5.3.4	电影评分数据分析案例

![image-20230808205956613](Image/image-20230808205956613.png)

```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import StructType, StringType, IntegerType


# 需求1. 用户平均分
def user_avg_score(df):
    df.groupBy('user_id') \
        .avg('rank') \
        .withColumnRenamed('avg(rank)', 'avg_rank') \
        .withColumn("avg_rank", F.round('avg_rank', 2)) \
        .orderBy("avg_rank", ascending=False) \
        .show()


# 需求2.电影平均分
def movie_avg_score(df):
    df.createTempView('movie')
    spark.sql("""
        SELECT movie_id, ROUND(AVG(rank), 2) AS avg_rank 
        FROM movie GROUP BY movie_id ORDER BY avg_rank DESC
    """).show()


# 需求3. 查询大于平均分的电影数量
def count_better_movie(df):
    # first()方法返回DataFrame的第一行, 是一个Row对象
    avg_score = df.select(F.avg('rank')).first()['avg(rank)']
    print('大于平均分的电影数量：', df.where(df['rank'] > avg_score).count())


# 需求4. 查询高分电影中(>3)打分次数最多的用户, 此人打分的平均分
def active_user_avg_score(df):
    active_user_id = df.where('rank > 3') \
        .groupBy('user_id') \
        .count() \
        .orderBy('count', ascending=False) \
        .limit(1) \
        .first()['user_id']
    df.where(df['user_id'] == active_user_id) \
        .select(F.round(F.avg('rank'), 2)).show()


# 需求5. 查询每个用户平均打分, 最低打分和最高打分
def stat_user_rank(df):
    df.groupBy('user_id') \
        .agg(
        F.round(F.avg('rank'), 2).alias('avg_rank'),
        F.min('rank').alias('min_rank'),
        F.max('rank').alias('max_rank')
    ).show()


# 6. 查询评分超过100次的电影 平均分、排名 TOP10
def stat_hot_movie(df):
    df.groupBy('movie_id') \
        .agg(
        F.count('movie_id').alias('count'),
        F.round(F.avg('movie_id'), 2).alias('avg_rank')
    ).where('count > 100') \
        .orderBy('avg_rank', ascending=False) \
        .limit(10) \
        .show()


if __name__ == '__main__':
    spark = SparkSession.builder \
        .appName('test') \
        .master('local[*]') \
        .getOrCreate()
    sc = spark.sparkContext

    # 读取数据
    schema = StructType().add('user_id', StringType()) \
        .add('movie_id', IntegerType()) \
        .add('rank', IntegerType()) \
        .add('ts', StringType())
    df = spark.read \
        .format('csv') \
        .options(sep='\t', header=False, encoding='utf-8') \
        .schema(schema) \
        .load('./data/u.data')

    # 1. 用户平均分
    user_avg_score(df)

    # 2. 电影平均分
    movie_avg_score(df)

    # 3. 查询大于平均分的电影数量
    count_better_movie(df)

    # 4. 查询高分电影中(>3)打分次数最多的用户, 此人打分的平均分
    active_user_avg_score(df)

    # 5. 查询每个用户平均打分, 最低打分和最高打分
    stat_user_rank(df)

    # 6. 查询评分超过100次的电影 平均分、排名 TOP10
    stat_hot_movie(df)
```

这里使用了几个新的方法：

```python
# agg: GroupedData的API, 其中可以写多个聚合方法
# alias: Column的API, 可以对列重命名
# withColumnRenamed: DataFrame的API, 可以对其中的指定列改名
# orderBy: DataFrame的API, 可以升降序排序
# first: DataFrame的API, 返回值为Row对象

# Row对象是一个数组，可以通过row['列名']取出当前行中某一列具体数值 
```



## 5.4	SparkSQL Shuffle

在Spark WebUI上查看时会发现某一Stage会有200个Task：

![image-20230808225823932](Image/image-20230808225823932.png)

这是因为在SparkSQL中，当Job产生Shuffle时，默认分区数(spark.sql.shuffle.partitions)为200。

修改SparkSQL Shuffle分区数可以通过：

- 配置文件:  conf/spark-defaults.conf: spark.sql.shuffle.partitions 100
- 客户端提交： spark-submit --conf "spark.sql.shuffle.partitions=100"
- 代码设置: SparkSession.builder.config("spark.sql.shuffle.partitions"， "2")

spark.sql.shuffle.partitions参数指的是：在SQL计算中，shuffle算子默认分区数为200，在集群模式下200个还可以，但local模式下就会增加很大的调度开销。这个参数与SparkRDD中，设置并行度的参数是独立的。



## 5.5	SparkSQL 数据清洗

在实际生产环境中，要先对初始数据进行数据清洗，将raw数据处理为符合后续处理要求的规整数据。

```python
# 去重

# dropDuplicates()方法将合并重复数据
df.dropDuplicates().show()
# 也可以按字段去重
df.dropDuplicates(['age', 'job']).show()

# 缺失值处理

# 删除有空值的行
df.dropna().show()
# 删除不足3个有效列的行
df.drop(thresh=3).show()
# 删除不足3个有效列的行, 只对subset内的列检查
df.drop(thresh=3, subset=['name', 'age', 'job']).show()

# 缺失值填充

df.fillna('loss').show()
# 指定列填充
df.fillna('N/A', subset=['job']).show()
df.fillna({
    'job': 'N/A',
    'age': 0,
    'name': 'unknown'
}).show()
```



## 5.6	SparkSQL 数据写出

SparkSQL的数据写出方法与数据读取方法格式类似：

```python
df.write\
.mode(append|overwrite|ignore|error)\
.format(text|csv|json|parquet|orc|avro|jdbc)\	# 注意text源只支持单列DataFrame写出
.option(K, V)\
.save(PATH)	# 写出路径, 支持本地文件和HDFS
```

Text数据写出：

```python
# 只能写出一个列的数据, 需要将DataFrame转换为单列
df.select(F.concat_ws('---', 'user_id', 'movie_id', 'rank', 'ts'))\
	.write\
	.
```

